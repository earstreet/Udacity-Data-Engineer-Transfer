{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Create a DWH on AWS with Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-necessary-modules\" data-toc-modified-id=\"Import-necessary-modules-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import necessary modules</a></span></li><li><span><a href=\"#Prepare-a-redshift-cluster-if-it-does-not-already-exist\" data-toc-modified-id=\"Prepare-a-redshift-cluster-if-it-does-not-already-exist-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prepare a redshift cluster if it does not already exist</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-DWH-params-from-config-file\" data-toc-modified-id=\"Load-DWH-params-from-config-file-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Load DWH params from config file</a></span></li><li><span><a href=\"#Create-clients-for-EC2,-S3,-IAM-and-Redshift\" data-toc-modified-id=\"Create-clients-for-EC2,-S3,-IAM-and-Redshift-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Create clients for EC2, S3, IAM and Redshift</a></span></li><li><span><a href=\"#Create-IAM-Role\" data-toc-modified-id=\"Create-IAM-Role-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Create IAM Role</a></span></li><li><span><a href=\"#Create-redshift-cluster\" data-toc-modified-id=\"Create-redshift-cluster-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Create redshift cluster</a></span></li><li><span><a href=\"#Check-cluster-status\" data-toc-modified-id=\"Check-cluster-status-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>Check cluster status</a></span></li><li><span><a href=\"#Get-endpoint-and-role-ARN-of-cluster\" data-toc-modified-id=\"Get-endpoint-and-role-ARN-of-cluster-1.2.6\"><span class=\"toc-item-num\">1.2.6&nbsp;&nbsp;</span>Get endpoint and role ARN of cluster</a></span></li><li><span><a href=\"#Open-an-incoming--TCP-port-to-access-the-cluster-endpoint\" data-toc-modified-id=\"Open-an-incoming--TCP-port-to-access-the-cluster-endpoint-1.2.7\"><span class=\"toc-item-num\">1.2.7&nbsp;&nbsp;</span>Open an incoming  TCP port to access the cluster endpoint</a></span></li><li><span><a href=\"#Check-connection-to-cluster\" data-toc-modified-id=\"Check-connection-to-cluster-1.2.8\"><span class=\"toc-item-num\">1.2.8&nbsp;&nbsp;</span>Check connection to cluster</a></span></li></ul></li></ul></li><li><span><a href=\"#Connect-to-redshift-cluster\" data-toc-modified-id=\"Connect-to-redshift-cluster-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Connect to redshift cluster</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-params-from-config-file\" data-toc-modified-id=\"Get-params-from-config-file-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get params from config file</a></span></li></ul></li><li><span><a href=\"#Clean-cluster-and-stage-data\" data-toc-modified-id=\"Clean-cluster-and-stage-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clean cluster and stage data</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQL-Queries-to-drop-tables\" data-toc-modified-id=\"SQL-Queries-to-drop-tables-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>SQL-Queries to drop tables</a></span></li><li><span><a href=\"#SQL-Queries-for-staging-tables\" data-toc-modified-id=\"SQL-Queries-for-staging-tables-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>SQL-Queries for staging tables</a></span></li><li><span><a href=\"#Create-tables\" data-toc-modified-id=\"Create-tables-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Create tables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Manage-all-queries-in-lists\" data-toc-modified-id=\"Manage-all-queries-in-lists-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Manage all queries in lists</a></span></li><li><span><a href=\"#Initiate-process-on-cluster\" data-toc-modified-id=\"Initiate-process-on-cluster-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Initiate process on cluster</a></span></li></ul></li></ul></li><li><span><a href=\"#Copy-S3-data-to-our-staging-tables-on-cluster\" data-toc-modified-id=\"Copy-S3-data-to-our-staging-tables-on-cluster-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Copy S3 data to our staging tables on cluster</a></span><ul class=\"toc-item\"><li><span><a href=\"#SQL-Queries-to-copy-data\" data-toc-modified-id=\"SQL-Queries-to-copy-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>SQL-Queries to copy data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Manage-all-queries-in-lists\" data-toc-modified-id=\"Manage-all-queries-in-lists-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Manage all queries in lists</a></span></li><li><span><a href=\"#Initiate-process-on-cluster\" data-toc-modified-id=\"Initiate-process-on-cluster-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Initiate process on cluster</a></span></li><li><span><a href=\"#Have-a-look-quick-at-the-data\" data-toc-modified-id=\"Have-a-look-quick-at-the-data-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Have a look quick at the data</a></span></li></ul></li></ul></li><li><span><a href=\"#Define-target-tables\" data-toc-modified-id=\"Define-target-tables-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define target tables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-user-table-and-insert-query\" data-toc-modified-id=\"Create-user-table-and-insert-query-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Create user table and insert query</a></span></li><li><span><a href=\"#Create-song-table-and-insert-query\" data-toc-modified-id=\"Create-song-table-and-insert-query-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Create song table and insert query</a></span></li><li><span><a href=\"#Create-artist-table-and-insert-query\" data-toc-modified-id=\"Create-artist-table-and-insert-query-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Create artist table and insert query</a></span></li><li><span><a href=\"#Create-time-table-and-insert-query\" data-toc-modified-id=\"Create-time-table-and-insert-query-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Create time table and insert query</a></span></li><li><span><a href=\"#Create-songplay-table-and-insert-query\" data-toc-modified-id=\"Create-songplay-table-and-insert-query-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Create songplay table and insert query</a></span></li></ul></li><li><span><a href=\"#Insert-data-into-tables\" data-toc-modified-id=\"Insert-data-into-tables-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Insert data into tables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Manage-all-queries-in-lists\" data-toc-modified-id=\"Manage-all-queries-in-lists-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Manage all queries in lists</a></span></li><li><span><a href=\"#Initiate-process-on-cluster\" data-toc-modified-id=\"Initiate-process-on-cluster-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Initiate process on cluster</a></span></li></ul></li><li><span><a href=\"#Validation-and-Performance-Testing\" data-toc-modified-id=\"Validation-and-Performance-Testing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Validation and Performance Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Size-and-content\" data-toc-modified-id=\"Size-and-content-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Size and content</a></span></li><li><span><a href=\"#Structure-and-Performance\" data-toc-modified-id=\"Structure-and-Performance-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Structure and Performance</a></span></li></ul></li><li><span><a href=\"#Clean-up-resources\" data-toc-modified-id=\"Clean-up-resources-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Clean up resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#Drop-tables\" data-toc-modified-id=\"Drop-tables-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Drop tables</a></span></li><li><span><a href=\"#Delete-cluster\" data-toc-modified-id=\"Delete-cluster-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Delete cluster</a></span></li><li><span><a href=\"#Delete-IAM-role\" data-toc-modified-id=\"Delete-IAM-role-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Delete IAM role</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a redshift cluster if it does not already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DWH params from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh-create.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "DWH_REGION             = config.get(\"DWH\",\"DWH_REGION\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "df_config = pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \n",
    "                   \"DWH_DB\", \"DWH_DB_USER\", \"DWH_PORT\", \"DWH_REGION\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, \n",
    "                   DWH_DB_USER, DWH_PORT, DWH_REGION, DWH_IAM_ROLE_NAME]\n",
    "             })\n",
    "df_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create clients for EC2, S3, IAM and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                     region_name = DWH_REGION,\n",
    "                     aws_access_key_id = KEY,\n",
    "                     aws_secret_access_key = SECRET\n",
    "                    )\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                   aws_access_key_id = KEY,\n",
    "                   aws_secret_access_key = SECRET,\n",
    "                   region_name = DWH_REGION\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name = DWH_REGION,\n",
    "                       aws_access_key_id = KEY,\n",
    "                       aws_secret_access_key = SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/code-samples/latest/catalog/python-iam-create_role.py.html\n",
    "try:\n",
    "    print('1.1 Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(Path='/',\n",
    "                         RoleName = DWH_IAM_ROLE_NAME,\n",
    "                         Description = 'Allows Redshift clusters to call AWS services on your behalf.',\n",
    "                         AssumeRolePolicyDocument=json.dumps(\n",
    "                             {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                                            'Effect':'Allow',\n",
    "                                            'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                             'Version': '2012-10-17'})\n",
    "                         )\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# https://docs.aws.amazon.com/code-samples/latest/catalog/python-iam-attach_role_policy.py.html\n",
    "print('1.2 Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # parameters for hardware\n",
    "        ClusterType = DWH_CLUSTER_TYPE,\n",
    "        NodeType = DWH_NODE_TYPE,\n",
    "        NumberOfNodes = int(DWH_NUM_NODES),\n",
    "        #Port = int(DWH_PORT),\n",
    "\n",
    "        # parameters for identifiers & credentials\n",
    "        DBName = DWH_DB,\n",
    "        MasterUsername = DWH_DB_USER,\n",
    "        MasterUserPassword = DWH_DB_PASSWORD,\n",
    "        ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "        \n",
    "        # parameter for role (to allow s3 access)\n",
    "        IamRoles = [roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check cluster status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "def checkAvailabilityRedshiftCluster(ClusterIdentifier):\n",
    "    import time\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=ClusterIdentifier)['Clusters'][0]\n",
    "    while myClusterProps['ClusterStatus'] != 'available':\n",
    "        print('Cluster-Status: %s. Waiting for 15s' % myClusterProps['ClusterStatus'])\n",
    "        time.sleep(15)\n",
    "        myClusterProps = redshift.describe_clusters(ClusterIdentifier=ClusterIdentifier)['Clusters'][0]\n",
    "    print('RedshiftCluster %s available!' % ClusterIdentifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkAvailabilityRedshiftCluster(DWH_CLUSTER_IDENTIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get endpoint and role ARN of cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open an incoming  TCP port to access the cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName='default',#'redshift_security_group',\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check connection to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT, DWH_DB)\n",
    "# print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to redshift cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get params from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "CLUSTER_HOST       = DWH_ENDPOINT\n",
    "DB_NAME            = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER            = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD        = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT            = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "DB_REGION          = config.get(\"CLUSTER\",\"DB_REGION\")\n",
    "\n",
    "IAM_ROLE_ARN       = config.get(\"IAM_ROLE\",\"ARN\")\n",
    "\n",
    "LOG_DATA            = config.get(\"S3\",\"LOG_DATA\")\n",
    "LOG_JSONPATH        = config.get(\"S3\",\"LOG_JSONPATH\")\n",
    "SONG_DATA           = config.get(\"S3\",\"SONG_DATA\")\n",
    "\n",
    "df_config = pd.DataFrame({\n",
    "              \"Param\":\n",
    "                  [\"CLUSTER_HOST\", \"DB_NAME\", \"DB_USER\", \"DB_PORT\", \n",
    "                   \"DB_REGION\", \"IAM_ROLE_ARN\", \"LOG_DATA\", \"LOG_JSONPATH\", \"SONG_DATA\"],\n",
    "              \"Value\":\n",
    "                  [CLUSTER_HOST, DB_NAME, DB_USER, DB_PORT, \n",
    "                   DB_REGION, IAM_ROLE_ARN, LOG_DATA, LOG_JSONPATH, SONG_DATA]\n",
    "             })\n",
    "df_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean cluster and stage data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL-Queries to drop tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP TABLES\n",
    "\n",
    "staging_events_table_drop = \"DROP TABLE IF EXISTS staging_events\"\n",
    "staging_songs_table_drop = \"DROP TABLE IF EXISTS staging_songs\"\n",
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists\"\n",
    "time_table_drop = \"DROP TABLE IF EXISTS time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL-Queries for staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_events_table_create= (\"\"\"CREATE TABLE IF NOT EXISTS staging_events (\n",
    "artist TEXT,\n",
    "auth TEXT,\n",
    "firstName TEXT,\n",
    "gender TEXT,\n",
    "itemInSession INTEGER,\n",
    "lastName TEXT,\n",
    "length FLOAT,\n",
    "level TEXT,\n",
    "location TEXT,\n",
    "method TEXT,\n",
    "page TEXT,\n",
    "registration TEXT,\n",
    "sessionId INTEGER,\n",
    "song TEXT,\n",
    "status INTEGER,\n",
    "ts TIMESTAMP,\n",
    "userAgent TEXT,\n",
    "userId INTEGER)\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"CREATE TABLE IF NOT EXISTS staging_songs (\n",
    "num_songs INTEGER, \n",
    "artist_id TEXT, \n",
    "artist_latitude FLOAT, \n",
    "artist_longitude FLOAT, \n",
    "artist_location TEXT, \n",
    "artist_name TEXT, \n",
    "song_id TEXT, \n",
    "title TEXT, \n",
    "duration FLOAT, \n",
    "year INTEGER)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        # show query for debugging\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        # show query for debugging\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manage all queries in lists\n",
    "ATTENTION: Run cell if create or drop table queries have been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all queries of one type in a list\n",
    "\n",
    "create_table_queries = [staging_events_table_create, staging_songs_table_create]\n",
    "drop_table_queries = [staging_events_table_drop, staging_songs_table_drop, songplay_table_drop, \n",
    "                      user_table_drop, song_table_drop, artist_table_drop, time_table_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate process on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"host={CLUSTER_HOST} dbname={DB_NAME} user={DB_USER} port={DB_PORT}\\n\")\n",
    "conn = psycopg2.connect(f\"host={CLUSTER_HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "drop_tables(cur, conn)\n",
    "create_tables(cur, conn)\n",
    "\n",
    "conn.close()\n",
    "print('Tables created and connection closed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy S3 data to our staging tables on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging_tables(cur, conn):\n",
    "    for query in copy_table_queries:\n",
    "        # show query for debugging\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL-Queries to copy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-run-copy.html\n",
    "# STAGING TABLES\n",
    "staging_events_copy = (f\"\"\"COPY staging_events \n",
    "FROM '{LOG_DATA}' \n",
    "IAM_ROLE '{IAM_ROLE_ARN}' \n",
    "REGION '{DB_REGION}'\n",
    "JSON '{LOG_JSONPATH}'\n",
    "TIMEFORMAT 'epochmillisecs';\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_copy = (f\"\"\"COPY staging_songs \n",
    "FROM '{SONG_DATA}' \n",
    "IAM_ROLE '{IAM_ROLE_ARN}' \n",
    "REGION '{DB_REGION}'\n",
    "JSON 'auto';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manage all queries in lists\n",
    "ATTENTION: Run cell if copy table queries have been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all copy queries in a list\n",
    "copy_table_queries = [staging_events_copy, staging_songs_copy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate process on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"host={CLUSTER_HOST} dbname={DB_NAME} user={DB_USER} port={DB_PORT}\\n\")\n",
    "conn = psycopg2.connect(f\"host={CLUSTER_HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "load_staging_tables(cur, conn)\n",
    "\n",
    "conn.close()\n",
    "print('All data copied and connection closed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look quick at the data\n",
    "Check length of tables and structure of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM staging_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM staging_events LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM staging_songs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM staging_songs LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show size of tables:\n",
    "https://docs.aws.amazon.com/redshift/latest/dg/tutorial-tuning-tables-test-performance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select stv_tbl_perm.name as table, count(*) as mb\n",
    "from stv_blocklist, stv_tbl_perm\n",
    "where stv_blocklist.tbl = stv_tbl_perm.id\n",
    "and stv_blocklist.slice = stv_tbl_perm.slice\n",
    "and stv_tbl_perm.name in ('staging_events','staging_songs')\n",
    "group by stv_tbl_perm.name\n",
    "order by 1 asc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show error table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM stl_load_errors;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user table and insert query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "user_id varchar PRIMARY KEY sortkey, \n",
    "first_name varchar NOT NULL, \n",
    "last_name varchar NOT NULL, \n",
    "gender varchar NOT NULL, \n",
    "level varchar NOT NULL)\n",
    "DISTSTYLE ALL;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test insert query\n",
    "%sql SELECT DISTINCT userid, firstname, lastname, gender, level FROM staging_events WHERE userid IS NOT NULL LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table_insert = (\"\"\"\n",
    "INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
    "SELECT DISTINCT userid, firstname, lastname, gender, level \n",
    "FROM staging_events\n",
    "WHERE userid IS NOT NULL;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create song table and insert query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songs (\n",
    "song_id varchar PRIMARY KEY sortkey, \n",
    "title varchar NOT NULL, \n",
    "artist_id varchar NOT NULL, \n",
    "year int NOT NULL, \n",
    "duration numeric NOT NULL)\n",
    "DISTSTYLE ALL;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test insert query\n",
    "%sql SELECT DISTINCT song_id, title, artist_id, year, duration FROM staging_songs LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_table_insert = (\"\"\"\n",
    "INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
    "SELECT DISTINCT song_id, title, artist_id, year, duration \n",
    "FROM staging_songs;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create artist table and insert query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artists (\n",
    "artist_id varchar PRIMARY KEY sortkey, \n",
    "name varchar NOT NULL, \n",
    "location varchar NOT NULL, \n",
    "latitude numeric NOT NULL, \n",
    "longitude numeric NOT NULL)\n",
    "DISTSTYLE ALL;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "-- Test insert query\n",
    "SELECT DISTINCT artist_id, artist_name, \n",
    "    ISNULL(artist_location, 'NaN') AS artist_location, \n",
    "    ISNULL(artist_latitude, 0.0) AS artist_latitude, \n",
    "    ISNULL(artist_longitude,0.0) AS artist_longitude\n",
    "FROM staging_songs \n",
    "WHERE artist_id IS NOT NULL \n",
    "ORDER BY artist_id\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table_insert = (\"\"\"\n",
    "INSERT INTO artists (artist_id, name, location, latitude, longitude)\n",
    "SELECT DISTINCT artist_id, artist_name, \n",
    "    ISNULL(artist_location, 'NaN') AS location, \n",
    "    ISNULL(artist_latitude, 0.0) AS latitude, \n",
    "    ISNULL(artist_longitude,0.0) AS longitude\n",
    "FROM staging_songs \n",
    "WHERE artist_id IS NOT NULL;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time table and insert query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS time (\n",
    "start_time timestamp PRIMARY KEY sortkey distkey, \n",
    "hour int NOT NULL, \n",
    "day int NOT NULL, \n",
    "week int NOT NULL, \n",
    "month int NOT NULL, \n",
    "year int NOT NULL, \n",
    "weekday varchar NOT NULL);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation about used functions:\n",
    "- https://docs.aws.amazon.com/redshift/latest/dg/r_DATE_PART_function.html\n",
    "- https://docs.aws.amazon.com/redshift/latest/dg/r_CAST_function.html\n",
    "\n",
    "Shorten datetime to hour in order to reduce the length of the time table according to the needs of the defined tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Test insert query\n",
    "SELECT DISTINCT DATE_TRUNC('hour', ts) AS start_time,\n",
    "CAST(DATE_PART('hour', ts) as Integer) AS hour,\n",
    "CAST(DATE_PART('day', ts) as Integer) AS day,\n",
    "CAST(DATE_PART('week', ts) as Integer) AS week,\n",
    "CAST(DATE_PART('month', ts) as Integer) AS month,\n",
    "CAST(DATE_PART('year', ts) as Integer) AS year,\n",
    "CAST(DATE_PART('weekday', ts) as Integer) AS weekday\n",
    "FROM staging_events \n",
    "ORDER BY ts\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table_insert = (\"\"\"\n",
    "INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
    "SELECT DISTINCT DATE_TRUNC('hour', ts) AS start_time,\n",
    "CAST(DATE_PART('hour', ts) as Integer) AS hour,\n",
    "CAST(DATE_PART('day', ts) as Integer) AS day,\n",
    "CAST(DATE_PART('week', ts) as Integer) AS week,\n",
    "CAST(DATE_PART('month', ts) as Integer) AS month,\n",
    "CAST(DATE_PART('year', ts) as Integer) AS year,\n",
    "CAST(DATE_PART('weekday', ts) as Integer) AS weekday\n",
    "FROM staging_events;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create songplay table and insert query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplays (\n",
    "songplay_id int IDENTITY(0,1) PRIMARY KEY, \n",
    "start_time timestamp NOT NULL REFERENCES time sortkey distkey, \n",
    "user_id varchar NOT NULL REFERENCES users, \n",
    "level varchar NOT NULL, \n",
    "song_id varchar NOT NULL REFERENCES songs, \n",
    "artist_id varchar NOT NULL REFERENCES artists, \n",
    "session_id varchar NOT NULL, \n",
    "location varchar NOT NULL, \n",
    "user_agent varchar NOT NULL);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Test insert query\n",
    "SELECT DATE_TRUNC('hour', se.ts), se.userid, se.level, ss.song_id, ss.artist_id, se.sessionid, se.location, se.useragent \n",
    "FROM staging_events se\n",
    "INNER JOIN staging_songs ss\n",
    "ON se.song = ss.title\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_table_insert = (\"\"\"\n",
    "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id,\n",
    "                         session_id, location, user_agent)\n",
    "SELECT DATE_TRUNC('hour', se.ts), se.userid, se.level, ss.song_id, ss.artist_id, se.sessionid, \n",
    "        se.location, se.useragent \n",
    "FROM staging_events se\n",
    "INNER JOIN staging_songs ss\n",
    "ON se.song = ss.title;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tables(cur, conn):\n",
    "    for query in insert_table_queries:\n",
    "        # show query for debugging\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage all queries in lists\n",
    "ATTENTION: Run cell if insert table queries have been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all queries of one type in a list\n",
    "create_table_queries = [user_table_create, song_table_create, artist_table_create, time_table_create, songplay_table_create]\n",
    "insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert, time_table_insert]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate process on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"host={CLUSTER_HOST} dbname={DB_NAME} user={DB_USER} port={DB_PORT}\\n\")\n",
    "conn = psycopg2.connect(f\"host={CLUSTER_HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "create_tables(cur, conn)\n",
    "insert_tables(cur, conn)\n",
    "\n",
    "conn.close()\n",
    "print('Tables created, data inserted and connection closed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Performance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM songplays;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM songplays LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM users ORDER BY user_id LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM songs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM songs LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM artists;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM artists LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM time;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM time LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- show size of tables\n",
    "select stv_tbl_perm.name as table, count(*) as mb\n",
    "from stv_blocklist, stv_tbl_perm\n",
    "where stv_blocklist.tbl = stv_tbl_perm.id\n",
    "and stv_blocklist.slice = stv_tbl_perm.slice\n",
    "and stv_tbl_perm.name in ('staging_events','staging_songs', 'songplays', 'users', 'songs', 'artists', 'time')\n",
    "group by stv_tbl_perm.name\n",
    "order by 2 desc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for errors\n",
    "%sql SELECT * FROM stl_load_errors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM PG_CLASS_INFO WHERE relname in ('staging_events','staging_songs', 'songplays', 'users', 'songs', 'artists', 'time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- check distribution style of tables\n",
    "select \"schema\", \"table\", diststyle from SVV_TABLE_INFO\n",
    "where \"table\" in ('songplays', 'users', 'songs', 'artists', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT sp.user_id, u.last_name, count(distinct sp.song_id) AS songs_played, \n",
    "    count(distinct sp.artist_id) AS artists_played\n",
    "FROM songplays sp\n",
    "JOIN users u\n",
    "ON sp.user_id = u.user_id\n",
    "JOIN songs s\n",
    "ON sp.song_id = s.song_id\n",
    "JOIN time t\n",
    "ON sp.start_time = t.start_time\n",
    "JOIN artists a\n",
    "ON sp.artist_id = a.artist_id\n",
    "WHERE t.month = 11\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 3 DESC, 4 DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "EXPLAIN\n",
    "SELECT sp.user_id, u.last_name, count(distinct sp.song_id) AS songs_played, \n",
    "        count(distinct sp.artist_id) AS artists_played\n",
    "FROM songplays sp\n",
    "JOIN users u\n",
    "ON sp.user_id = u.user_id\n",
    "JOIN songs s\n",
    "ON sp.song_id = s.song_id\n",
    "JOIN time t\n",
    "ON sp.start_time = t.start_time\n",
    "JOIN artists a\n",
    "ON sp.artist_id = a.artist_id\n",
    "WHERE t.month = 11\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 3 DESC, 4 DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- drop the staging tables\n",
    "DROP TABLE IF EXISTS staging_events;\n",
    "DROP TABLE IF EXISTS staging_songs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- drop the productive tables\n",
    "DROP TABLE IF EXISTS songplays;\n",
    "DROP TABLE IF EXISTS users;\n",
    "DROP TABLE IF EXISTS songs;\n",
    "DROP TABLE IF EXISTS artists;\n",
    "DROP TABLE IF EXISTS time;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check status of cluster\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDeletionRedshiftCluster(ClusterIdentifier):\n",
    "    import time\n",
    "    try:\n",
    "        myClusterProps = redshift.describe_clusters(ClusterIdentifier=ClusterIdentifier)['Clusters'][0]\n",
    "        while myClusterProps['ClusterStatus'] == 'deleting':\n",
    "            print('Cluster-Status: %s. Waiting for 15s' % myClusterProps['ClusterStatus'])\n",
    "            time.sleep(15)\n",
    "            myClusterProps = redshift.describe_clusters(ClusterIdentifier=ClusterIdentifier)['Clusters'][0]\n",
    "    except:\n",
    "        print('RedshiftCluster %s sucessfully deleted!' % ClusterIdentifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDeletionRedshiftCluster(DWH_CLUSTER_IDENTIFIER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the created IAM role\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
