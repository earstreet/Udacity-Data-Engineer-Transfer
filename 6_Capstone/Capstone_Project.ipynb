{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration to U.S. \n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "## Project Summary\n",
    "In this project I connect different data sources to give users the possibility to analyse immigration to the U.S. according Temperature and U.S. City Demographic Data.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "### Configure environment and start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import configparser\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import split, udf, col, lower\n",
    "from pyspark.sql.types import IntegerType\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure pandas settings\n",
    "pd.set_option(\"display.max_columns\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging format\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get params from config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# input_data = \"./\"\n",
    "# output_data = \"s3a://dend-capstone-project\"\n",
    "input_data = './data/'\n",
    "output_data = '/Users/daniel/Desktop/output/'\n",
    "# output_data = 'C:/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .getOrCreate()\n",
    "    spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "*Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc*\n",
    "\n",
    "The main purpose of the project is to link different data sources. To do this, it is important to preprocess the data, find and filter out errors in the data, replace missing entries meaningfully or, if necessary, remove them.\n",
    "\n",
    "I use the data sets provided by Udacity from various sources.\n",
    "\n",
    "In the end, a data structure should emerge which enables the user to carry out his own questions and analyses. To create this, I will use the tools and services I have learned. In particular, I will use Spark to process the data and generate the data tables. Furthermore, I will use the infrastructure of AWS to use more suitable data processing and deployment services, independent of the local computer.\n",
    "\n",
    "### Describe and Gather Data \n",
    "*Describe the data sets you're using. Where did it come from? What type of information is included?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape of spark dataframe\n",
    "def spark_shape(df):\n",
    "    return (df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I94 Immigration Data\n",
    "\n",
    "This data comes from the [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html) as sas-File. A smaller dataset is given as parquet-files. It contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). The complete dataset has a size of 40.790.529 rows and 34 columns.\n",
    "\n",
    "In addition to the data files, Udacity offers a description file containing explanations of the individual parameters. Some of the codes used, for example for cities, states and countries, are decoded.\n",
    "\n",
    "##### Smaller immigration test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/sas_data/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load smaller test table from parquet files\n",
    "immigration_data_small = os.path.join(input_data, 'sas_data/')\n",
    "immigration_data_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_small_df = spark.read.parquet(immigration_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3096313, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_shape(immigration_small_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full table from sas files \n",
    "immigration_data_path = os.path.join(input_data, 'data/18-83510-I94-Data-2016/*.sas7bdat')\n",
    "immigration_data = [f for f in glob.glob(immigration_data_path)]\n",
    "immigration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function according https://stackoverflow.com/a/56666722\n",
    "# Keep all columns in either df1 or df2\n",
    "def outter_union(df1, df2):\n",
    "\n",
    "    # Add missing columns to df1\n",
    "    left_df = df1\n",
    "    for column in set(df2.columns) - set(df1.columns):\n",
    "        left_df = left_df.withColumn(column, F.lit(None))\n",
    "\n",
    "    # Add missing columns to df2\n",
    "    right_df = df2\n",
    "    for column in set(df1.columns) - set(df2.columns):\n",
    "        right_df = right_df.withColumn(column, F.lit(None))\n",
    "\n",
    "    return left_df.unionByName(right_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in immigration_data:\n",
    "    if file == immigration_data[0]:\n",
    "        immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    else:\n",
    "        new_df = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "        immigration_df = outter_union(immigration_df, new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: full immigration table has up to 40 million rows (slow)\n",
    "spark_shape(immigration_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse description file\n",
    "i94_desc_path = os.path.join(input_data, 'I94_SAS_Labels_Descriptions.SAS')\n",
    "with open(i94_desc_path) as f:\n",
    "    lines = f.readlines()\n",
    "    i94_desc_string = ''.join(lines)\n",
    "    i94_desc_string = i94_desc_string.replace('\\n', '')\n",
    "    i94_desc_string = i94_desc_string.replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I94YR - 4 digit year \n",
      "I94MON - Numeric month \n",
      "I94CIT & I94RES - This format shows all the valid and invalid codes for processing \n",
      "I94PORT - This format shows all the valid and invalid codes for processing \n",
      "ARRDATE is the Arrival Date in the USA. It is a SAS date numeric field that a permament format has not been applied. Please apply whichever date format works for you. \n",
      "I94MODE - There are missing values as well as not reported (9) \n",
      "I94ADDR - There is lots of invalid codes in this variable and the list below shows what we have found to be valid, everything else goes into 'other' \n",
      "DEPDATE is the Departure Date from the USA. It is a SAS date numeric field that a permament format has not been applied. Please apply whichever date format works for you. \n",
      "I94BIR - Age of Respondent in Years \n",
      "I94VISA - Visa codes collapsed into three categories: 1 = Business 2 = Pleasure 3 = Student\n",
      "COUNT - Used for summary statistics \n",
      "DTADFILE - Character Date Field - Date added to I-94 Files - CIC does not use \n",
      "VISAPOST - Department of State where where Visa was issued - CIC does not use \n",
      "OCCUP - Occupation that will be performed in U.S. - CIC does not use \n",
      "ENTDEPA - Arrival Flag - admitted or paroled into the U.S. - CIC does not use \n",
      "ENTDEPD - Departure Flag - Departed, lost I-94 or is deceased - CIC does not use \n",
      "ENTDEPU - Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use \n",
      "MATFLAG - Match flag - Match of arrival and departure records \n",
      "BIRYEAR - 4 digit year of birth \n",
      "DTADDTO - Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use \n",
      "GENDER - Non-immigrant sex \n",
      "INSNUM - INS number \n",
      "AIRLINE - Airline used to arrive in U.S. \n",
      "ADMNUM - Admission Number \n",
      "FLTNO - Flight number of Airline used to arrive in U.S. \n",
      "VISATYPE - Class of admission legally admitting the non-immigrant to temporarily stay in U.S. \n"
     ]
    }
   ],
   "source": [
    "# use Regular Expressions to extract the necessary data\n",
    "i94_param = re.findall(r'\\/\\*\\s+(.*?)\\*\\/', i94_desc_string)\n",
    "    \n",
    "for item in i94_param:\n",
    "    # replace multiple whitespaces\n",
    "    item = re.sub('\\s+',' ',item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "|---|---|\n",
    "| I94YR | 4 digit year |\n",
    "| I94MON | Numeric month |\n",
    "| I94CIT | 3 digit code for the country of birth of the immigrant |\n",
    "| I94RES | 3 digit code for the resident country of the immigrant |\n",
    "| I94PORT | Port of arrival |\n",
    "| ARRDATE | Arrival Date in the USA |\n",
    "| I94MODE | Mode of transportation: 1 = Air; 2 = Sea; 3 = Land; 9 = Not reported |\n",
    "| I94ADDR | State Code of arrival |\n",
    "| DEPDATE | Departure Date from the USA |\n",
    "| I94BIR | Age of Respondent in Years |\n",
    "| I94VISA | Visa codes collapsed into three categories: 1 = Business 2 = Pleasure 3 = Student|\n",
    "| COUNT | Used for summary statistics |\n",
    "| DTADFILE | Character Date Field |\n",
    "| VISAPOST | Department of State where where Visa was issued |\n",
    "| OCCUP | Occupation that will be performed in U.S. |\n",
    "| ENTDEPA | Arrival Flag - admitted or paroled into the U.S. |\n",
    "| ENTDEPD | Departure Flag - Departed, lost I-94 or is deceased |\n",
    "| ENTDEPU | Update Flag - Either apprehended, overstayed, adjusted to perm residence |\n",
    "| MATFLAG | Match flag - Match of arrival and departure records |\n",
    "| BIRYEAR | 4 digit year of birth |\n",
    "| DTADDTO | Character Date Field - Date to which admitted to U.S. (allowed to stay until) |\n",
    "| GENDER | Non-immigrant sex |\n",
    "| INSNUM | INS number |\n",
    "| AIRLINE | Airline used to arrive in U.S. |\n",
    "| ADMNUM | Admission Number |\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. |\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Temperature Data\n",
    "\n",
    "This dataset comes from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) as csv-File. It contains information about the global land temperature by state and starts in 1750 for average land temperature. The dataset has a size of 645.675 rows and 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/GlobalLandTemperaturesByState.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = os.path.join(input_data, 'GlobalLandTemperaturesByState.csv')\n",
    "temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.read.csv(temp_data, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645675, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_shape(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "|---|---|\n",
    "| dt | Timestamp in YYYY-MM-DD |\n",
    "| AverageTemperature | Global Average Land Temperature by City |\n",
    "| AverageTemperatureUncertainty | 95% confidence interval around the average |\n",
    "| State | Name of State |\n",
    "| Country | Name of Country |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U.S. City Demographic Data\n",
    "\n",
    "This data comes from [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) as a csv-File. It contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. It comes from the US Census Bureau's 2015 American Community Survey. The dataset has a size of 2.891 rows and 12 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/us-cities-demographics.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_data = os.path.join(input_data, 'us-cities-demographics.csv')\n",
    "cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = spark.read.csv(cities_data, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_shape(cities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "|---|---|\n",
    "| City | Name of City |\n",
    "| State | US state of the city |\n",
    "| Median Age | Median age of the population |\n",
    "| Male Population | Number of male population |\n",
    "| Female Population | Number of female population |\n",
    "| Total Population | Number of total population |\n",
    "| Number of Veterans | Number of veterans in the city |\n",
    "| Foreign-born | Number of citizens that were not born in the city |\n",
    "| Average Household Size | Average number of people living in a house in the city |\n",
    "| State Code | Code of the state |\n",
    "| Race | Race class |\n",
    "| Count | Number of people for each race |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Code Table\n",
    "\n",
    "This data comes from [datahub.io](https://datahub.io/core/airport-codes#data) as a csv-File. It is a simple table of airport codes and corresponding cities. The dataset has a size of 55.075 rows and 12 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/airport-codes_csv.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_data = os.path.join(input_data, 'airport-codes_csv.csv')\n",
    "airports_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = spark.read.csv(airports_data, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_shape(airports_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Description |\n",
    "|---|---|\n",
    "| ident | Unique identifier |\n",
    "| type | Type of airport |\n",
    "| name | Name of Airport |\n",
    "| elevation_ft | Altitude of airport |\n",
    "| continent | Continent |\n",
    "| iso_country | ISO code of the country of the airport |\n",
    "| iso_region | ISO code for the region of the airport |\n",
    "| municipality | City of airport |\n",
    "| gps_code | GPS code of airport |\n",
    "| iata_code | IATA code of airport |\n",
    "| local_code | Local code of airport |\n",
    "| coordinates | GPS coordinates of airport |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write raw tables as parquet files to S3\n",
    "\n",
    "Store raw data from the internet to S3 bucket as parquet files for backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all tables into one dictionary\n",
    "df_raw_all = {'immigration_small_data': immigration_small_df,\n",
    "            #'immigration_data': immigration_df, \n",
    "            'temp_data': temp_df, \n",
    "            'cities_data': cities_df,\n",
    "            'airport_data': airports_df\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, df in df_raw_all.items():\n",
    "    # remove spaces and minus in column-names of dataframes\n",
    "    df = df.select([F.col(col).alias(col.replace(' ', '_').replace('-', '_')) for col in df.columns])\n",
    "    # write parquet files to raw path\n",
    "    path = os.path.join(output_data, 'raw', file)\n",
    "    df.write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore data from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-18 18:08:44,934 - Dataframe <immigration_small_data> from parquet-file in </Users/daniel/Desktop/output/raw/immigration_small_data/> successfully loaded\n",
      "2020-03-18 18:08:45,063 - Dataframe <airport_data> from parquet-file in </Users/daniel/Desktop/output/raw/airport_data/> successfully loaded\n",
      "2020-03-18 18:08:45,215 - Dataframe <temp_data> from parquet-file in </Users/daniel/Desktop/output/raw/temp_data/> successfully loaded\n",
      "2020-03-18 18:08:45,339 - Dataframe <cities_data> from parquet-file in </Users/daniel/Desktop/output/raw/cities_data/> successfully loaded\n"
     ]
    }
   ],
   "source": [
    "raw_path = os.path.join(output_data, 'raw')\n",
    "table_names = [y for x, y, z in os.walk(raw_path)][0]\n",
    "subdirs = glob.glob(raw_path + '/*/')\n",
    "df_raw_all = {}\n",
    "for name, path in zip(table_names, subdirs):\n",
    "    df_raw_all[name] = spark.read.parquet(path)\n",
    "    logging.info(f'Dataframe <{name}> from parquet-file in <{path}> successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace large immigration dataframe with the smaller one for testing on local computer\n",
    "df_raw_all['immigration_data'] = df_raw_all['immigration_small_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for exploring\n",
    "\n",
    "# count NULL values in columns\n",
    "def na_cols_in_df(df):\n",
    "    total_rows = df.count()\n",
    "    table = {}\n",
    "    for col in df.columns:\n",
    "        total_na = df.where(f\"{col} is NULL\").count()\n",
    "        percent_na = total_na / total_rows * 100\n",
    "        table[col] = [total_na, percent_na]\n",
    "    table = pd.DataFrame(table, index=['Total NA', 'Percent NA'])\n",
    "    # round each number by 2 decimals\n",
    "    table = table.round(2)\n",
    "    print(f'Total number of rows: {total_rows}')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>SYD</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748522.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.498180e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5748523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497969e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5748524.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497975e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5748525.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497325e+10</td>\n",
       "      <td>00028</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5748526.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ACK</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.501355e+10</td>\n",
       "      <td>00002</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  5748517.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "1  5748518.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "2  5748519.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "3  5748520.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "4  5748521.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "5  5748522.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "6  5748523.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "7  5748524.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "8  5748525.0  2016.0     4.0   245.0   464.0     HOU  20574.0      1.0   \n",
       "9  5748526.0  2016.0     4.0   245.0   464.0     LOS  20574.0      1.0   \n",
       "\n",
       "  i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa  \\\n",
       "0      CA  20582.0    40.0      1.0    1.0  20160430      SYD  None       G   \n",
       "1      NV  20591.0    32.0      1.0    1.0  20160430      SYD  None       G   \n",
       "2      WA  20582.0    29.0      1.0    1.0  20160430      SYD  None       G   \n",
       "3      WA  20588.0    29.0      1.0    1.0  20160430      SYD  None       G   \n",
       "4      WA  20588.0    28.0      1.0    1.0  20160430      SYD  None       G   \n",
       "5      HI  20579.0    57.0      2.0    1.0  20160430      ACK  None       G   \n",
       "6      HI  20586.0    66.0      2.0    1.0  20160430      ACK  None       G   \n",
       "7      HI  20586.0    41.0      2.0    1.0  20160430      ACK  None       G   \n",
       "8      FL  20581.0    27.0      2.0    1.0  20160430      ACK  None       G   \n",
       "9      CA  20581.0    26.0      2.0    1.0  20160430      ACK  None       G   \n",
       "\n",
       "  entdepd entdepu matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0       O    None       M   1976.0  10292016      F   None      QF   \n",
       "1       O    None       M   1984.0  10292016      F   None      VA   \n",
       "2       O    None       M   1987.0  10292016      M   None      DL   \n",
       "3       O    None       M   1987.0  10292016      F   None      DL   \n",
       "4       O    None       M   1988.0  10292016      M   None      DL   \n",
       "5       O    None       M   1959.0  10292016      M   None      NZ   \n",
       "6       O    None       M   1950.0  10292016      F   None      NZ   \n",
       "7       O    None       M   1975.0  10292016      F   None      NZ   \n",
       "8       O    None       M   1989.0  10292016      M   None      NZ   \n",
       "9       O    None       M   1990.0  10292016      F   None      NZ   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  9.495387e+10  00011       B1  \n",
       "1  9.495562e+10  00007       B1  \n",
       "2  9.495641e+10  00040       B1  \n",
       "3  9.495645e+10  00040       B1  \n",
       "4  9.495639e+10  00040       B1  \n",
       "5  9.498180e+10  00010       B2  \n",
       "6  9.497969e+10  00010       B2  \n",
       "7  9.497975e+10  00010       B2  \n",
       "8  9.497325e+10  00028       B2  \n",
       "9  9.501355e+10  00002       B2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_all['immigration_data'].limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 3096313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total NA</th>\n",
       "      <th>Percent NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cicid</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94yr</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mon</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94cit</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94res</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94port</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrdate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mode</th>\n",
       "      <td>239.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94addr</th>\n",
       "      <td>152592.0</td>\n",
       "      <td>4.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depdate</th>\n",
       "      <td>142457.0</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94bir</th>\n",
       "      <td>802.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94visa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtadfile</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visapost</th>\n",
       "      <td>1881250.0</td>\n",
       "      <td>60.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occup</th>\n",
       "      <td>3088187.0</td>\n",
       "      <td>99.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepa</th>\n",
       "      <td>238.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepd</th>\n",
       "      <td>138429.0</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entdepu</th>\n",
       "      <td>3095921.0</td>\n",
       "      <td>99.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matflag</th>\n",
       "      <td>138429.0</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biryear</th>\n",
       "      <td>802.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtaddto</th>\n",
       "      <td>477.0</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>414269.0</td>\n",
       "      <td>13.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insnum</th>\n",
       "      <td>2982605.0</td>\n",
       "      <td>96.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>83627.0</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admnum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fltno</th>\n",
       "      <td>19549.0</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visatype</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Total NA  Percent NA\n",
       "cicid           0.0        0.00\n",
       "i94yr           0.0        0.00\n",
       "i94mon          0.0        0.00\n",
       "i94cit          0.0        0.00\n",
       "i94res          0.0        0.00\n",
       "i94port         0.0        0.00\n",
       "arrdate         0.0        0.00\n",
       "i94mode       239.0        0.01\n",
       "i94addr    152592.0        4.93\n",
       "depdate    142457.0        4.60\n",
       "i94bir        802.0        0.03\n",
       "i94visa         0.0        0.00\n",
       "count           0.0        0.00\n",
       "dtadfile        1.0        0.00\n",
       "visapost  1881250.0       60.76\n",
       "occup     3088187.0       99.74\n",
       "entdepa       238.0        0.01\n",
       "entdepd    138429.0        4.47\n",
       "entdepu   3095921.0       99.99\n",
       "matflag    138429.0        4.47\n",
       "biryear       802.0        0.03\n",
       "dtaddto       477.0        0.02\n",
       "gender     414269.0       13.38\n",
       "insnum    2982605.0       96.33\n",
       "airline     83627.0        2.70\n",
       "admnum          0.0        0.00\n",
       "fltno       19549.0        0.63\n",
       "visatype        0.0        0.00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_in_df(df_raw_all['immigration_data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot's of missing data in some columns. I will focus the work on the columns with the most data available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1976-05-01</td>\n",
       "      <td>16.558999999999994</td>\n",
       "      <td>0.307</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1976-06-01</td>\n",
       "      <td>22.253</td>\n",
       "      <td>0.26</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1976-07-01</td>\n",
       "      <td>24.023000000000003</td>\n",
       "      <td>0.325</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976-08-01</td>\n",
       "      <td>23.02</td>\n",
       "      <td>0.185</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1976-09-01</td>\n",
       "      <td>19.52</td>\n",
       "      <td>0.181</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1976-10-01</td>\n",
       "      <td>11.659</td>\n",
       "      <td>0.276</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1976-11-01</td>\n",
       "      <td>4.643</td>\n",
       "      <td>0.122</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1976-12-01</td>\n",
       "      <td>2.062</td>\n",
       "      <td>0.182</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>-4.7650000000000015</td>\n",
       "      <td>0.126</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1977-02-01</td>\n",
       "      <td>3.482</td>\n",
       "      <td>0.244</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt   AverageTemperature AverageTemperatureUncertainty      State  \\\n",
       "0  1976-05-01   16.558999999999994                         0.307  Tennessee   \n",
       "1  1976-06-01               22.253                          0.26  Tennessee   \n",
       "2  1976-07-01   24.023000000000003                         0.325  Tennessee   \n",
       "3  1976-08-01                23.02                         0.185  Tennessee   \n",
       "4  1976-09-01                19.52                         0.181  Tennessee   \n",
       "5  1976-10-01               11.659                         0.276  Tennessee   \n",
       "6  1976-11-01                4.643                         0.122  Tennessee   \n",
       "7  1976-12-01                2.062                         0.182  Tennessee   \n",
       "8  1977-01-01  -4.7650000000000015                         0.126  Tennessee   \n",
       "9  1977-02-01                3.482                         0.244  Tennessee   \n",
       "\n",
       "         Country  \n",
       "0  United States  \n",
       "1  United States  \n",
       "2  United States  \n",
       "3  United States  \n",
       "4  United States  \n",
       "5  United States  \n",
       "6  United States  \n",
       "7  United States  \n",
       "8  United States  \n",
       "9  United States  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_all['temp_data'].limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 645675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total NA</th>\n",
       "      <th>Percent NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AverageTemperature</th>\n",
       "      <td>25648.0</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <td>25648.0</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Total NA  Percent NA\n",
       "dt                                  0.0        0.00\n",
       "AverageTemperature              25648.0        3.97\n",
       "AverageTemperatureUncertainty   25648.0        3.97\n",
       "State                               0.0        0.00\n",
       "Country                             0.0        0.00"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_in_df(df_raw_all['temp_data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Temperature Data is missing. These rows should be dropped in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median_Age</th>\n",
       "      <th>Male_Population</th>\n",
       "      <th>Female_Population</th>\n",
       "      <th>Total_Population</th>\n",
       "      <th>Number_of_Veterans</th>\n",
       "      <th>Foreign_born</th>\n",
       "      <th>Average_Household_Size</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229</td>\n",
       "      <td>62432</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634</td>\n",
       "      <td>7517</td>\n",
       "      <td>2.4</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712</td>\n",
       "      <td>41971</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815</td>\n",
       "      <td>8355</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629</td>\n",
       "      <td>56860</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800</td>\n",
       "      <td>37038</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762</td>\n",
       "      <td>43270</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783</td>\n",
       "      <td>3269</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751</td>\n",
       "      <td>58077</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204</td>\n",
       "      <td>16315</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City           State Median_Age Male_Population  \\\n",
       "0     Silver Spring        Maryland       33.8           40601   \n",
       "1            Quincy   Massachusetts       41.0           44129   \n",
       "2            Hoover         Alabama       38.5           38040   \n",
       "3  Rancho Cucamonga      California       34.5           88127   \n",
       "4            Newark      New Jersey       34.6          138040   \n",
       "5            Peoria        Illinois       33.1           56229   \n",
       "6          Avondale         Arizona       29.1           38712   \n",
       "7       West Covina      California       39.8           51629   \n",
       "8          O'Fallon        Missouri       36.0           41762   \n",
       "9        High Point  North Carolina       35.5           51751   \n",
       "\n",
       "  Female_Population Total_Population Number_of_Veterans Foreign_born  \\\n",
       "0             41862            82463               1562        30908   \n",
       "1             49500            93629               4147        32935   \n",
       "2             46799            84839               4819         8229   \n",
       "3             87105           175232               5821        33878   \n",
       "4            143873           281913               5829        86253   \n",
       "5             62432           118661               6634         7517   \n",
       "6             41971            80683               4815         8355   \n",
       "7             56860           108489               3800        37038   \n",
       "8             43270            85032               5783         3269   \n",
       "9             58077           109828               5204        16315   \n",
       "\n",
       "  Average_Household_Size State_Code                               Race  Count  \n",
       "0                    2.6         MD                 Hispanic or Latino  25924  \n",
       "1                   2.39         MA                              White  58723  \n",
       "2                   2.58         AL                              Asian   4759  \n",
       "3                   3.18         CA          Black or African-American  24437  \n",
       "4                   2.73         NJ                              White  76402  \n",
       "5                    2.4         IL  American Indian and Alaska Native   1343  \n",
       "6                   3.18         AZ          Black or African-American  11592  \n",
       "7                   3.56         CA                              Asian  32716  \n",
       "8                   2.77         MO                 Hispanic or Latino   2583  \n",
       "9                   2.65         NC                              Asian  11060  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_all['cities_data'].limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 2891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total NA</th>\n",
       "      <th>Percent NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median_Age</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male_Population</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female_Population</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Population</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number_of_Veterans</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foreign_born</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average_Household_Size</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Code</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Total NA  Percent NA\n",
       "City                         0.0        0.00\n",
       "State                        0.0        0.00\n",
       "Median_Age                   0.0        0.00\n",
       "Male_Population              3.0        0.10\n",
       "Female_Population            3.0        0.10\n",
       "Total_Population             0.0        0.00\n",
       "Number_of_Veterans          13.0        0.45\n",
       "Foreign_born                13.0        0.45\n",
       "Average_Household_Size      16.0        0.55\n",
       "State_Code                   0.0        0.00\n",
       "Race                         0.0        0.00\n",
       "Count                        0.0        0.00"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_in_df(df_raw_all['cities_data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Code Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>None</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>None</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00AS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fulton Airport</td>\n",
       "      <td>1100</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-OK</td>\n",
       "      <td>Alex</td>\n",
       "      <td>00AS</td>\n",
       "      <td>None</td>\n",
       "      <td>00AS</td>\n",
       "      <td>-97.8180194, 34.9428028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00AZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Cordes Airport</td>\n",
       "      <td>3810</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>Cordes</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>None</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>-112.16500091552734, 34.305599212646484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00CA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goldstone /Gts/ Airport</td>\n",
       "      <td>3038</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Barstow</td>\n",
       "      <td>00CA</td>\n",
       "      <td>None</td>\n",
       "      <td>00CA</td>\n",
       "      <td>-116.888000488, 35.350498199499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00CL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Williams Ag Airport</td>\n",
       "      <td>87</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Biggs</td>\n",
       "      <td>00CL</td>\n",
       "      <td>None</td>\n",
       "      <td>00CL</td>\n",
       "      <td>-121.763427, 39.427188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00CN</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kitchen Creek Helibase Heliport</td>\n",
       "      <td>3350</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Pine Valley</td>\n",
       "      <td>00CN</td>\n",
       "      <td>None</td>\n",
       "      <td>00CN</td>\n",
       "      <td>-116.4597417, 32.7273736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport           11   \n",
       "1  00AA  small_airport                Aero B Ranch Airport         3435   \n",
       "2  00AK  small_airport                        Lowell Field          450   \n",
       "3  00AL  small_airport                        Epps Airpark          820   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport          237   \n",
       "5  00AS  small_airport                      Fulton Airport         1100   \n",
       "6  00AZ  small_airport                      Cordes Airport         3810   \n",
       "7  00CA  small_airport             Goldstone /Gts/ Airport         3038   \n",
       "8  00CL  small_airport                 Williams Ag Airport           87   \n",
       "9  00CN       heliport     Kitchen Creek Helibase Heliport         3350   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0        NA          US      US-PA      Bensalem      00A      None   \n",
       "1        NA          US      US-KS         Leoti     00AA      None   \n",
       "2        NA          US      US-AK  Anchor Point     00AK      None   \n",
       "3        NA          US      US-AL       Harvest     00AL      None   \n",
       "4        NA          US      US-AR       Newport     None      None   \n",
       "5        NA          US      US-OK          Alex     00AS      None   \n",
       "6        NA          US      US-AZ        Cordes     00AZ      None   \n",
       "7        NA          US      US-CA       Barstow     00CA      None   \n",
       "8        NA          US      US-CA         Biggs     00CL      None   \n",
       "9        NA          US      US-CA   Pine Valley     00CN      None   \n",
       "\n",
       "  local_code                              coordinates  \n",
       "0        00A       -74.93360137939453, 40.07080078125  \n",
       "1       00AA                   -101.473911, 38.704022  \n",
       "2       00AK              -151.695999146, 59.94919968  \n",
       "3       00AL    -86.77030181884766, 34.86479949951172  \n",
       "4       None                      -91.254898, 35.6087  \n",
       "5       00AS                  -97.8180194, 34.9428028  \n",
       "6       00AZ  -112.16500091552734, 34.305599212646484  \n",
       "7       00CA       -116.888000488, 35.350498199499995  \n",
       "8       00CL                   -121.763427, 39.427188  \n",
       "9       00CN                 -116.4597417, 32.7273736  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_all['airport_data'].limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 55075\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total NA</th>\n",
       "      <th>Percent NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ident</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevation_ft</th>\n",
       "      <td>7006.0</td>\n",
       "      <td>12.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>continent</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iso_country</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iso_region</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>municipality</th>\n",
       "      <td>5676.0</td>\n",
       "      <td>10.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gps_code</th>\n",
       "      <td>14045.0</td>\n",
       "      <td>25.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iata_code</th>\n",
       "      <td>45886.0</td>\n",
       "      <td>83.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_code</th>\n",
       "      <td>26389.0</td>\n",
       "      <td>47.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinates</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Total NA  Percent NA\n",
       "ident              0.0        0.00\n",
       "type               0.0        0.00\n",
       "name               0.0        0.00\n",
       "elevation_ft    7006.0       12.72\n",
       "continent          0.0        0.00\n",
       "iso_country        0.0        0.00\n",
       "iso_region         0.0        0.00\n",
       "municipality    5676.0       10.31\n",
       "gps_code       14045.0       25.50\n",
       "iata_code      45886.0       83.32\n",
       "local_code     26389.0       47.91\n",
       "coordinates        0.0        0.00"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols_in_df(df_raw_all['airport_data']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_all['airport_data'].groupBy(['name', 'type', 'coordinates']).count().where('count > 1').orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_all['airport_data'].where(\"name = 'Mukho Port Heliport'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_all['airport_data'].where(\"name LIKE 'Cheonmi-ri South%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some airports in the dataset with duplicate data. These rows should be dropped in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "*Document steps necessary to clean the data*\n",
    "\n",
    "* Temperature data will only be used for 2012 which is the last complete dataset and the nearest to the immigrations dataset from 2016\n",
    "* Remove data from the 2016s immigration dataset which has a timecode before 2016\n",
    "* Drop duplicates in all data tables\n",
    "* Remove nulls and inconsistent data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "*Map out the conceptual data model and explain why you chose that model*\n",
    "\n",
    "#### Table links\n",
    "![Image](images/table_links.png)\n",
    "\n",
    "To get an overview of possible links between the individual data tables, it is useful to choose a graphical representation. From this, it is much easier to derive a decision for a later data model.\n",
    "\n",
    "It is easy to see that there are many links between the individual tables, for example to connect the country and city columns. \n",
    "\n",
    "#### Choosen data model\n",
    "![Image](images/star_schema.png)\n",
    "\n",
    "I think that a star schema with one fact and several dimension tables fits very well to this application. The Immigrations table is a classic fact table with numerous single events that can be analyzed by the user. The dimension tables, which provide additional information on date, city, state and temperature are structured around this table. In the case of temperature data, I choose a division into a (entry) country table and a US state table. With this it is possible to perform analyses that consider where the traveler comes from as well as where he goes.\n",
    "\n",
    "I left out the airport table because the data for the exercise do not provide any added value. Only the column `elevation_ft` is added as additional information. However, for our observation case I do not see any useful addition to the exercise.\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "*List the steps necessary to pipeline the data into the chosen data model*\n",
    "\n",
    "1. Staging of the necessary tables\n",
    "2. Creating the fact and dimension tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "*Build the data pipelines to create the data model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Immigrations table\n",
    "#### Get country codes from SAS description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|country_id|             country|\n",
      "+----------+--------------------+\n",
      "|       582|mexico air sea, a...|\n",
      "|       236|         afghanistan|\n",
      "|       101|             albania|\n",
      "|       316|             algeria|\n",
      "|       102|             andorra|\n",
      "|       324|              angola|\n",
      "|       529|            anguilla|\n",
      "|       518|     antigua-barbuda|\n",
      "|       687|          argentina |\n",
      "|       151|             armenia|\n",
      "|       532|               aruba|\n",
      "|       438|           australia|\n",
      "|       103|             austria|\n",
      "|       152|          azerbaijan|\n",
      "|       512|             bahamas|\n",
      "|       298|             bahrain|\n",
      "|       274|          bangladesh|\n",
      "|       513|            barbados|\n",
      "|       104|             belgium|\n",
      "|       581|              belize|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_country_codes = dict(re.findall(r'(\\d{3})\\s+\\=\\s+\\'(.*?)\\'', i94_desc_string))\n",
    "#i94_country_codes\n",
    "lol = list(map(list, i94_country_codes.items()))\n",
    "countries_table = spark.createDataFrame(lol, [\"country_id\", \"country\"])\n",
    "countries_table = countries_table.withColumn('country', lower(col('country')))\n",
    "countries_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_data, 'staging', 'countries_table')\n",
    "countries_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get state codes from SAS description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|state_id|            state|\n",
      "+--------+-----------------+\n",
      "|      AL|          alabama|\n",
      "|      AK|           alaska|\n",
      "|      AZ|          arizona|\n",
      "|      AR|         arkansas|\n",
      "|      CA|       california|\n",
      "|      CO|         colorado|\n",
      "|      CT|      connecticut|\n",
      "|      DE|         delaware|\n",
      "|      DC|dist. of columbia|\n",
      "|      FL|          florida|\n",
      "|      GA|          georgia|\n",
      "|      GU|             guam|\n",
      "|      HI|           hawaii|\n",
      "|      ID|            idaho|\n",
      "|      IL|         illinois|\n",
      "|      IN|          indiana|\n",
      "|      IA|             iowa|\n",
      "|      KS|           kansas|\n",
      "|      KY|         kentucky|\n",
      "|      LA|        louisiana|\n",
      "+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_state_codes = dict(re.findall(r\"'(\\w{2})'\\s*=\\s*'(.*?)\\s*'\", i94_desc_string))\n",
    "# remove port keys\n",
    "port_keys = ['AG', 'NK']\n",
    "for key in port_keys:\n",
    "    del i94_state_codes[key]\n",
    "#i94_state_codes\n",
    "lol = list(map(list, i94_state_codes.items()))\n",
    "states_staging = spark.createDataFrame(lol, [\"state_id\", \"state\"])\n",
    "states_staging = states_staging.withColumn('state', lower(col('state')))\n",
    "states_staging.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_data, 'staging', 'states_table')\n",
    "states_staging.write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Mode and Visa codes from SAS description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94_mode_codes = {\n",
    "    1 : 'Air',\n",
    "    2 : 'Sea',\n",
    "    3 : 'Land',\n",
    "    9 : 'Not reported'\n",
    "}\n",
    "i94_visa_codes = {\n",
    "    1 : 'Business',\n",
    "    2 : 'Pleasure',\n",
    "    3 : 'Student'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|mode_id|        mode|\n",
      "+-------+------------+\n",
      "|      1|         Air|\n",
      "|      2|         Sea|\n",
      "|      3|        Land|\n",
      "|      9|Not reported|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lol = list(map(list, i94_mode_codes.items()))\n",
    "modes_table = spark.createDataFrame(lol, [\"mode_id\", \"mode\"])\n",
    "modes_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_data, 'staging', 'modes_table')\n",
    "modes_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|visa_id|    visa|\n",
      "+-------+--------+\n",
      "|      1|Business|\n",
      "|      2|Pleasure|\n",
      "|      3| Student|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lol = list(map(list, i94_visa_codes.items()))\n",
    "visas_table = spark.createDataFrame(lol, [\"visa_id\", \"visa\"])\n",
    "visas_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_data, 'staging', 'visas_table')\n",
    "visas_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Immigration table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrations_stage = df_raw_all['immigration_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all kind of None strings with Spark Null\n",
    "immigrations_stage = immigrations_stage.replace(['NaN', 'NONE', 'Null', 'null', 'None'],[None, None, None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first day in 2016 as SAS date\n",
    "start_date = abs(datetime(2016, 1, 1).date() - datetime(1960, 1, 1).date()).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with dates before 2016\n",
    "immigrations_stage = immigrations_stage.filter(immigrations_stage.arrdate >= start_date).filter(immigrations_stage.depdate >= start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp from sas dates\n",
    "get_timestamp = udf(lambda x: (datetime(1960, 1, 1).date() + timedelta(x)).isoformat() if x else None)\n",
    "immigrations_stage = immigrations_stage.withColumn('arrdate_ts', get_timestamp(immigrations_stage.arrdate))\n",
    "immigrations_stage = immigrations_stage.withColumn('depdate_ts', get_timestamp(immigrations_stage.depdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mode columns\n",
    "immigrations_stage = immigrations_stage.join(modes_table, modes_table.mode_id == immigrations_stage.i94mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add visa columns\n",
    "immigrations_stage = immigrations_stage.join(visas_table, visas_table.visa_id == immigrations_stage.i94visa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add country names to i94cit and i94res\n",
    "immigrations_stage = immigrations_stage.join(countries_table, countries_table.country_id == immigrations_stage.i94cit).withColumnRenamed('country','i94cit_country').drop('country_id')\n",
    "immigrations_stage = immigrations_stage.join(countries_table, countries_table.country_id == immigrations_stage.i94res).withColumnRenamed('country','i94res_country').drop('country_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "      <th>arrdate_ts</th>\n",
       "      <th>depdate_ts</th>\n",
       "      <th>mode_id</th>\n",
       "      <th>mode</th>\n",
       "      <th>visa_id</th>\n",
       "      <th>visa</th>\n",
       "      <th>i94cit_country</th>\n",
       "      <th>i94res_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5761453.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DC</td>\n",
       "      <td>20580.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ULN</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>KE</td>\n",
       "      <td>9.496416e+10</td>\n",
       "      <td>00093</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-06</td>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>mongolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5761458.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ULN</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>KE</td>\n",
       "      <td>9.496449e+10</td>\n",
       "      <td>00093</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-14</td>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>mongolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5761467.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ULN</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>UA</td>\n",
       "      <td>9.498118e+10</td>\n",
       "      <td>00892</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>mongolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5761472.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ULN</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>UA</td>\n",
       "      <td>9.498081e+10</td>\n",
       "      <td>00892</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>mongolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5761474.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>20595.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160430</td>\n",
       "      <td>ULN</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>UA</td>\n",
       "      <td>9.498075e+10</td>\n",
       "      <td>00892</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>mongolia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  5761453.0  2016.0     4.0   299.0   299.0     WAS  20574.0      1.0   \n",
       "1  5761458.0  2016.0     4.0   299.0   299.0     WAS  20574.0      1.0   \n",
       "2  5761467.0  2016.0     4.0   299.0   299.0     SFR  20574.0      1.0   \n",
       "3  5761472.0  2016.0     4.0   299.0   299.0     SFR  20574.0      1.0   \n",
       "4  5761474.0  2016.0     4.0   299.0   299.0     SFR  20574.0      1.0   \n",
       "\n",
       "  i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa  \\\n",
       "0      DC  20580.0    50.0      1.0    1.0  20160430      ULN  None       G   \n",
       "1      WA  20588.0    40.0      1.0    1.0  20160430      ULN  None       G   \n",
       "2      CA  20581.0    31.0      1.0    1.0  20160430      ULN  None       G   \n",
       "3      VA  20581.0    32.0      1.0    1.0  20160430      ULN  None       G   \n",
       "4      VA  20595.0    29.0      1.0    1.0  20160430      ULN  None       G   \n",
       "\n",
       "  entdepd entdepu matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0       O    None       M   1966.0  10292016      M   None      KE   \n",
       "1       O    None       M   1976.0  10292016      M   None      KE   \n",
       "2       O    None       M   1985.0  10292016      F   None      UA   \n",
       "3       O    None       M   1984.0  10292016      F   None      UA   \n",
       "4       O    None       M   1987.0  10292016      F   None      UA   \n",
       "\n",
       "         admnum  fltno visatype  arrdate_ts  depdate_ts  mode_id mode  \\\n",
       "0  9.496416e+10  00093       B1  2016-04-30  2016-05-06        1  Air   \n",
       "1  9.496449e+10  00093       B1  2016-04-30  2016-05-14        1  Air   \n",
       "2  9.498118e+10  00892       B1  2016-04-30  2016-05-07        1  Air   \n",
       "3  9.498081e+10  00892       B1  2016-04-30  2016-05-07        1  Air   \n",
       "4  9.498075e+10  00892       B1  2016-04-30  2016-05-21        1  Air   \n",
       "\n",
       "   visa_id      visa i94cit_country i94res_country  \n",
       "0        1  Business       mongolia       mongolia  \n",
       "1        1  Business       mongolia       mongolia  \n",
       "2        1  Business       mongolia       mongolia  \n",
       "3        1  Business       mongolia       mongolia  \n",
       "4        1  Business       mongolia       mongolia  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigrations_stage.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_data, 'staging', 'immigrations_stage')\n",
    "immigrations_stage.write.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrations_table = immigrations_stage.selectExpr(\n",
    "    'cast(cicid as int)', \n",
    "    'arrdate_ts', \n",
    "    'depdate_ts', \n",
    "    'i94port', \n",
    "    'cast(i94mode as int)',\n",
    "    'airline',\n",
    "    'fltno',\n",
    "    'cast(i94yr as int)',\n",
    "    'cast(i94mon as int)',\n",
    "    'visatype',\n",
    "    'visa AS i94visa',\n",
    "    'cast(i94bir as int)',\n",
    "    'i94cit_country AS i94cit',\n",
    "    'i94res_country AS i94res',\n",
    "    'i94addr',\n",
    "    'gender'\n",
    ").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>arrdate_ts</th>\n",
       "      <th>depdate_ts</th>\n",
       "      <th>i94port</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>airline</th>\n",
       "      <th>fltno</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>visatype</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5291801</td>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>2016-06-13</td>\n",
       "      <td>DAL</td>\n",
       "      <td>1</td>\n",
       "      <td>KE</td>\n",
       "      <td>00031</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>46</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>TX</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4111492</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>NYC</td>\n",
       "      <td>1</td>\n",
       "      <td>KE</td>\n",
       "      <td>00085</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B2</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>34</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>mongolia</td>\n",
       "      <td>MA</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621773</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-08</td>\n",
       "      <td>ORL</td>\n",
       "      <td>1</td>\n",
       "      <td>AV</td>\n",
       "      <td>00028</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>38</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>FL</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3478949</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>NYC</td>\n",
       "      <td>1</td>\n",
       "      <td>XL</td>\n",
       "      <td>00538</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>28</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>NY</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3479800</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>ATL</td>\n",
       "      <td>1</td>\n",
       "      <td>DL</td>\n",
       "      <td>00680</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>B1</td>\n",
       "      <td>Business</td>\n",
       "      <td>44</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>NH</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cicid  arrdate_ts  depdate_ts i94port  i94mode airline  fltno  i94yr  \\\n",
       "0  5291801  2016-04-28  2016-06-13     DAL        1      KE  00031   2016   \n",
       "1  4111492  2016-04-22  2016-04-23     NYC        1      KE  00085   2016   \n",
       "2   621773  2016-04-03  2016-04-08     ORL        1      AV  00028   2016   \n",
       "3  3478949  2016-04-18  2016-04-23     NYC        1      XL  00538   2016   \n",
       "4  3479800  2016-04-18  2016-04-24     ATL        1      DL  00680   2016   \n",
       "\n",
       "   i94mon visatype   i94visa  i94bir    i94cit    i94res i94addr gender  \n",
       "0       4       B2  Pleasure      46  mongolia  mongolia      TX   None  \n",
       "1       4       B2  Pleasure      34  mongolia  mongolia      MA      F  \n",
       "2       4       B1  Business      38   ecuador   ecuador      FL      M  \n",
       "3       4       B1  Business      28   ecuador   ecuador      NY      F  \n",
       "4       4       B1  Business      44   ecuador   ecuador      NH      M  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigrations_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- arrdate_ts: string (nullable = true)\n",
      " |-- depdate_ts: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigrations_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_data, 'ops', 'immigrations_table')\n",
    "immigrations_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Cities table\n",
    "#### Get city codes from SAS description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94_city_codes = dict(re.findall(r\"'(\\w{3})'\\s*=\\s*'(.*?)\\s*'\", i94_desc_string))\n",
    "#i94_city_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|city_id|          city_state|\n",
      "+-------+--------------------+\n",
      "|    ALC|           ALCAN, AK|\n",
      "|    ANC|       ANCHORAGE, AK|\n",
      "|    BAR|BAKER AAF - BAKER...|\n",
      "|    DAC|   DALTONS CACHE, AK|\n",
      "|    PIZ|DEW STATION PT LA...|\n",
      "|    DTH|    DUTCH HARBOR, AK|\n",
      "|    EGL|           EAGLE, AK|\n",
      "|    FRB|       FAIRBANKS, AK|\n",
      "|    HOM|           HOMER, AK|\n",
      "|    HYD|           HYDER, AK|\n",
      "|    JUN|          JUNEAU, AK|\n",
      "|    5KE|       KETCHIKAN, AK|\n",
      "|    KET|       KETCHIKAN, AK|\n",
      "|    MOS|MOSES POINT INTER...|\n",
      "|    NIK|         NIKISKI, AK|\n",
      "|    NOM|             NOM, AK|\n",
      "|    PKC|     POKER CREEK, AK|\n",
      "|    ORI|  PORT LIONS SPB, AK|\n",
      "|    SKA|         SKAGWAY, AK|\n",
      "|    SNP| ST. PAUL ISLAND, AK|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lol = list(map(list, i94_city_codes.items()))\n",
    "cities_table = spark.createDataFrame(lol, [\"city_id\", \"city_state\"])\n",
    "cities_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n",
      "|city_id|                city|state_code|\n",
      "+-------+--------------------+----------+\n",
      "|    ALC|               ALCAN|        AK|\n",
      "|    ANC|           ANCHORAGE|        AK|\n",
      "|    BAR|BAKER AAF - BAKER...|        AK|\n",
      "|    DAC|       DALTONS CACHE|        AK|\n",
      "|    PIZ|DEW STATION PT LA...|        AK|\n",
      "|    DTH|        DUTCH HARBOR|        AK|\n",
      "|    EGL|               EAGLE|        AK|\n",
      "|    FRB|           FAIRBANKS|        AK|\n",
      "|    HOM|               HOMER|        AK|\n",
      "|    HYD|               HYDER|        AK|\n",
      "|    JUN|              JUNEAU|        AK|\n",
      "|    5KE|           KETCHIKAN|        AK|\n",
      "|    KET|           KETCHIKAN|        AK|\n",
      "|    MOS|MOSES POINT INTER...|        AK|\n",
      "|    NIK|             NIKISKI|        AK|\n",
      "|    NOM|                 NOM|        AK|\n",
      "|    PKC|         POKER CREEK|        AK|\n",
      "|    ORI|      PORT LIONS SPB|        AK|\n",
      "|    SKA|             SKAGWAY|        AK|\n",
      "|    SNP|     ST. PAUL ISLAND|        AK|\n",
      "+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split city_state in two separate columns\n",
    "cities_table = cities_table.withColumn(\"city\", split(\"city_state\", \", \").getItem(0)).withColumn(\"state_code\", split(\"city_state\", \", \").getItem(1))\n",
    "cities_table = cities_table.drop('city_state')\n",
    "cities_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_data, 'ops', 'cities_table')\n",
    "cities_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 States table\n",
    "#### Get states data from Demographic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|         State|Median_Age|Male_Population|Female_Population|Total_Population|Number_of_Veterans|Foreign_born|Average_Household_Size|State_Code|                Race|Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...| 1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...|11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian|32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino| 2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian|11060|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw_all['cities_data'].limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median_Age: string (nullable = true)\n",
      " |-- Male_Population: string (nullable = true)\n",
      " |-- Female_Population: string (nullable = true)\n",
      " |-- Total_Population: string (nullable = true)\n",
      " |-- Number_of_Veterans: string (nullable = true)\n",
      " |-- Foreign_born: string (nullable = true)\n",
      " |-- Average_Household_Size: string (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw_all['cities_data'].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert necessary column to integer\n",
    "demographics_table = df_raw_all['cities_data'].withColumn(\"Male_Population\", df_raw_all['cities_data'][\"Male_Population\"].cast(IntegerType()))\\\n",
    "                                                .withColumn(\"Female_Population\", df_raw_all['cities_data'][\"Female_Population\"].cast(IntegerType()))\\\n",
    "                                                .withColumn(\"Total_Population\", df_raw_all['cities_data'][\"Total_Population\"].cast(IntegerType()))\\\n",
    "                                                .withColumn(\"Number_of_Veterans\", df_raw_all['cities_data'][\"Number_of_Veterans\"].cast(IntegerType()))\\\n",
    "                                                .withColumn(\"Foreign_born\", df_raw_all['cities_data'][\"Foreign_born\"].cast(IntegerType()))\\\n",
    "                                                .withColumn(\"Count\", df_raw_all['cities_data'][\"Count\"].cast(IntegerType()))\n",
    "# pivot race column, aggregate by sum for state view\n",
    "group_cols = ['State_Code', 'State', 'Male_Population', 'Female_Population', 'Total_Population', 'Number_of_Veterans', 'Foreign_born']\n",
    "demographics_table = demographics_table.groupBy(group_cols).pivot('Race').sum('Count')\n",
    "demographics_table = demographics_table.groupBy(['State_Code', 'State']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace blancs, 'sum' and brackets in column names\n",
    "demographics_table = demographics_table.select([F.col(col).alias(col.replace(' ', '_').replace('-','_').lower().replace(r'sum(', '').replace(r')', '')) for col in demographics_table.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>state</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>american_indian_and_alaska_native</th>\n",
       "      <th>asian</th>\n",
       "      <th>black_or_african_american</th>\n",
       "      <th>hispanic_or_latino</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>152945</td>\n",
       "      <td>145750</td>\n",
       "      <td>298695</td>\n",
       "      <td>27492</td>\n",
       "      <td>33258</td>\n",
       "      <td>36339</td>\n",
       "      <td>36825</td>\n",
       "      <td>23107</td>\n",
       "      <td>27261</td>\n",
       "      <td>212696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>497248</td>\n",
       "      <td>552381</td>\n",
       "      <td>1049629</td>\n",
       "      <td>71543</td>\n",
       "      <td>52154</td>\n",
       "      <td>8084</td>\n",
       "      <td>28769</td>\n",
       "      <td>521068</td>\n",
       "      <td>39313</td>\n",
       "      <td>498920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>286479</td>\n",
       "      <td>303400</td>\n",
       "      <td>589879</td>\n",
       "      <td>31704</td>\n",
       "      <td>62108</td>\n",
       "      <td>9381</td>\n",
       "      <td>22062</td>\n",
       "      <td>149608</td>\n",
       "      <td>77813</td>\n",
       "      <td>384733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2227455</td>\n",
       "      <td>2272087</td>\n",
       "      <td>4499542</td>\n",
       "      <td>264505</td>\n",
       "      <td>682313</td>\n",
       "      <td>129708</td>\n",
       "      <td>229183</td>\n",
       "      <td>296222</td>\n",
       "      <td>1508157</td>\n",
       "      <td>3591611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>12278281</td>\n",
       "      <td>12544179</td>\n",
       "      <td>24822460</td>\n",
       "      <td>928270</td>\n",
       "      <td>7448257</td>\n",
       "      <td>401386</td>\n",
       "      <td>4543730</td>\n",
       "      <td>2047009</td>\n",
       "      <td>9856464</td>\n",
       "      <td>14905129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CO</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>1454619</td>\n",
       "      <td>1481050</td>\n",
       "      <td>2935669</td>\n",
       "      <td>187896</td>\n",
       "      <td>337631</td>\n",
       "      <td>62613</td>\n",
       "      <td>148790</td>\n",
       "      <td>208043</td>\n",
       "      <td>703722</td>\n",
       "      <td>2463916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>432157</td>\n",
       "      <td>453424</td>\n",
       "      <td>885581</td>\n",
       "      <td>24953</td>\n",
       "      <td>225866</td>\n",
       "      <td>10729</td>\n",
       "      <td>48311</td>\n",
       "      <td>231822</td>\n",
       "      <td>309992</td>\n",
       "      <td>505674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DC</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>319705</td>\n",
       "      <td>352523</td>\n",
       "      <td>672228</td>\n",
       "      <td>25963</td>\n",
       "      <td>95117</td>\n",
       "      <td>6130</td>\n",
       "      <td>35072</td>\n",
       "      <td>328786</td>\n",
       "      <td>71129</td>\n",
       "      <td>285402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DE</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>32680</td>\n",
       "      <td>39277</td>\n",
       "      <td>71957</td>\n",
       "      <td>3063</td>\n",
       "      <td>3336</td>\n",
       "      <td>414</td>\n",
       "      <td>1193</td>\n",
       "      <td>44182</td>\n",
       "      <td>5516</td>\n",
       "      <td>23743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FL</td>\n",
       "      <td>Florida</td>\n",
       "      <td>3236773</td>\n",
       "      <td>3487375</td>\n",
       "      <td>6796738</td>\n",
       "      <td>388228</td>\n",
       "      <td>1688931</td>\n",
       "      <td>46759</td>\n",
       "      <td>264933</td>\n",
       "      <td>1652619</td>\n",
       "      <td>1942022</td>\n",
       "      <td>4758144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code                 state  male_population  female_population  \\\n",
       "0         AK                Alaska           152945             145750   \n",
       "1         AL               Alabama           497248             552381   \n",
       "2         AR              Arkansas           286479             303400   \n",
       "3         AZ               Arizona          2227455            2272087   \n",
       "4         CA            California         12278281           12544179   \n",
       "5         CO              Colorado          1454619            1481050   \n",
       "6         CT           Connecticut           432157             453424   \n",
       "7         DC  District of Columbia           319705             352523   \n",
       "8         DE              Delaware            32680              39277   \n",
       "9         FL               Florida          3236773            3487375   \n",
       "\n",
       "   total_population  number_of_veterans  foreign_born  \\\n",
       "0            298695               27492         33258   \n",
       "1           1049629               71543         52154   \n",
       "2            589879               31704         62108   \n",
       "3           4499542              264505        682313   \n",
       "4          24822460              928270       7448257   \n",
       "5           2935669              187896        337631   \n",
       "6            885581               24953        225866   \n",
       "7            672228               25963         95117   \n",
       "8             71957                3063          3336   \n",
       "9           6796738              388228       1688931   \n",
       "\n",
       "   american_indian_and_alaska_native    asian  black_or_african_american  \\\n",
       "0                              36339    36825                      23107   \n",
       "1                               8084    28769                     521068   \n",
       "2                               9381    22062                     149608   \n",
       "3                             129708   229183                     296222   \n",
       "4                             401386  4543730                    2047009   \n",
       "5                              62613   148790                     208043   \n",
       "6                              10729    48311                     231822   \n",
       "7                               6130    35072                     328786   \n",
       "8                                414     1193                      44182   \n",
       "9                              46759   264933                    1652619   \n",
       "\n",
       "   hispanic_or_latino     white  \n",
       "0               27261    212696  \n",
       "1               39313    498920  \n",
       "2               77813    384733  \n",
       "3             1508157   3591611  \n",
       "4             9856464  14905129  \n",
       "5              703722   2463916  \n",
       "6              309992    505674  \n",
       "7               71129    285402  \n",
       "8                5516     23743  \n",
       "9             1942022   4758144  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_table.orderBy('State_Code').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save states temperature table\n",
    "states_table = demographics_table.select('*')\n",
    "# write table to ops folder\n",
    "path = os.path.join(output_data, 'ops', 'states_table')\n",
    "states_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Temperature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_table = df_raw_all['temp_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(dt='2013-09-01')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures_table.select('dt').rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+---------+-------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|    State|      Country|\n",
      "+----------+------------------+-----------------------------+---------+-------------+\n",
      "|2012-01-01|             5.799|                        0.294|Tennessee|United States|\n",
      "|2012-02-01|7.0280000000000005|                         0.24|Tennessee|United States|\n",
      "|2012-03-01|15.655999999999999|          0.17600000000000002|Tennessee|United States|\n",
      "|2012-04-01|            15.826|                        0.254|Tennessee|United States|\n",
      "|2012-05-01|            21.705|                         0.17|Tennessee|United States|\n",
      "|2012-06-01|            23.524|                        0.281|Tennessee|United States|\n",
      "|2012-07-01|27.383000000000003|                        0.324|Tennessee|United States|\n",
      "|2012-08-01|             24.64|                        0.304|Tennessee|United States|\n",
      "|2012-09-01|20.968000000000004|                        0.203|Tennessee|United States|\n",
      "|2012-10-01|            13.975|                        0.282|Tennessee|United States|\n",
      "|2012-11-01|             8.008|          0.14400000000000002|Tennessee|United States|\n",
      "|2012-12-01| 7.657999999999999|                        0.304|Tennessee|United States|\n",
      "|2012-01-01|            10.127|                        0.238|    Texas|United States|\n",
      "|2012-02-01|            10.911|                         0.15|    Texas|United States|\n",
      "|2012-03-01|            17.261|                        0.155|    Texas|United States|\n",
      "|2012-04-01|             21.19|                        0.244|    Texas|United States|\n",
      "|2012-05-01|            24.288|          0.17600000000000002|    Texas|United States|\n",
      "|2012-06-01|            28.138|                        0.273|    Texas|United States|\n",
      "|2012-07-01|            28.809|                        0.259|    Texas|United States|\n",
      "|2012-08-01|28.941999999999997|                        0.266|    Texas|United States|\n",
      "+----------+------------------+-----------------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter table for the last complete year of the dataset which is in 2012\n",
    "temperatures_table = temperatures_table.where(\"dt LIKE '2012%'\")\n",
    "temperatures_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+---------+-------------+\n",
      "|year|month|average_temperature|    state|      country|\n",
      "+----+-----+-------------------+---------+-------------+\n",
      "|2012|    1|              5.799|Tennessee|United States|\n",
      "|2012|    2| 7.0280000000000005|Tennessee|United States|\n",
      "|2012|    3| 15.655999999999999|Tennessee|United States|\n",
      "|2012|    4|             15.826|Tennessee|United States|\n",
      "|2012|    5|             21.705|Tennessee|United States|\n",
      "+----+-----+-------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split date column to month and year because temperature in 2012 is only recorded monthly\n",
    "temperatures_table = temperatures_table.selectExpr(\n",
    "    'year(dt) AS year',\n",
    "    'month(dt) AS month',\n",
    "    'AverageTemperature AS average_temperature',\n",
    "    'State AS state',\n",
    "    'Country AS country'\n",
    ")\n",
    "temperatures_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save states temperature table\n",
    "us_state_temperatures_table = temperatures_table.select('*')\n",
    "# write table to ops folder\n",
    "path = os.path.join(output_data, 'ops', 'us_state_temperatures_table')\n",
    "us_state_temperatures_table.write.parquet(path, partitionBy=['country', 'state', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create countries temperature table\n",
    "country_temperatures_table = temperatures_table.select('*').groupBy(['Country', 'month', 'year']).agg(F.avg('average_temperature').alias('average_temperature'))\n",
    "# lower country names for join\n",
    "country_temperatures_table = country_temperatures_table.withColumn('Country', lower(col('Country')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write table to ops folder\n",
    "path = os.path.join(output_data, 'ops', 'country_temperatures_table')\n",
    "country_temperatures_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test join\n",
    "country_temperatures_table.join(countries_table, country_temperatures_table.Country == countries_table.country).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Dates table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrdate_table = immigrations_table.select('arrdate_ts').distinct()\n",
    "depdate_table = immigrations_table.select('depdate_ts').distinct()\n",
    "date_table = arrdate_table.union(depdate_table).distinct().orderBy('arrdate_ts')\n",
    "date_table = date_table.selectExpr(\n",
    "    'arrdate_ts AS date',\n",
    "    'day(arrdate_ts) AS day',\n",
    "    'month(arrdate_ts) AS month',\n",
    "    'year(arrdate_ts) AS year',\n",
    "    'weekofyear(arrdate_ts) AS week',\n",
    "    'weekday(arrdate_ts) AS weekday'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write table to ops folder\n",
    "path = os.path.join(output_data, 'ops', 'date_table')\n",
    "date_table.write.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+----+----+-------+\n",
      "|      date|day|month|year|week|weekday|\n",
      "+----------+---+-----+----+----+-------+\n",
      "|2016-01-03|  3|    1|2016|  53|      6|\n",
      "|2016-01-05|  5|    1|2016|   1|      1|\n",
      "|2016-01-06|  6|    1|2016|   1|      2|\n",
      "|2016-01-10| 10|    1|2016|   1|      6|\n",
      "|2016-01-13| 13|    1|2016|   2|      2|\n",
      "+----------+---+-----+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "*Explain the data quality checks you'll perform to ensure the pipeline ran as expected.*\n",
    "\n",
    "The Data Quality Check is divided into three parts. For this purpose, all existing data tables from the `dev` folder are first read in one after the other.\n",
    "\n",
    "In a first step before the import, it is checked whether the data table is part of the used star schema.\n",
    "\n",
    "Secondly, the number of rows is determined. If the table is empty, an error message is displayed.\n",
    "\n",
    "In a third step, the number of columns in the imported data table is determined and it is checked whether there is more than one column. If there is only one column or no column at all, an error message is also generated.\n",
    "\n",
    "The fourth step checks whether all imported data sources completely correspond to the previously defined star schema. If there are any discrepancies, this will also be displayed to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe <us_state_temperatures_table> successfully loaded.\n",
      "Data Quality Checks passed successfully. <us_state_temperatures_table> contains <2892> rows and <5> columns.\n",
      "\n",
      "Dataframe <cities_table> successfully loaded.\n",
      "Data Quality Checks passed successfully. <cities_table> contains <656> rows and <3> columns.\n",
      "\n",
      "Dataframe <states_table> successfully loaded.\n",
      "Data Quality Checks passed successfully. <states_table> contains <49> rows and <12> columns.\n",
      "\n",
      "Dataframe <country_temperatures_table> successfully loaded.\n",
      "Data Quality Checks passed successfully. <country_temperatures_table> contains <84> rows and <4> columns.\n",
      "\n",
      "Dataframe <date_table> successfully loaded.\n",
      "Data Quality Checks passed successfully. <date_table> contains <207> rows and <6> columns.\n",
      "\n",
      "Dataframe <immigrations_table> successfully loaded.\n",
      "Data Quality Checks passed successfully. <immigrations_table> contains <2587414> rows and <16> columns.\n",
      "\n",
      "All tables from star schema exist.\n"
     ]
    }
   ],
   "source": [
    "# check part of star_schema, number of rows and columns of tables in ops folder \n",
    "star_schema_tables = ['immigrations_table', \n",
    "                   'date_table', \n",
    "                   'cities_table', \n",
    "                   'states_table', \n",
    "                   'us_state_temperatures_table', \n",
    "                   'country_temperatures_table']\n",
    "found_tables = []\n",
    "\n",
    "path = os.path.join(output_data, 'ops')\n",
    "table_names = [y for x, y, z in os.walk(path)][0]\n",
    "subdirs = glob.glob(path + '/*/')\n",
    "df_check = {}\n",
    "for name, path in zip(table_names, subdirs):\n",
    "    # check part of schema\n",
    "    if name not in star_schema_tables:\n",
    "        raise ValueError(f'Table {name} not known from star schema.')\n",
    "    # load table from parquet\n",
    "    df_check[name] = spark.read.parquet(path)\n",
    "    print(f'Dataframe <{name}> successfully loaded.')\n",
    "    # count rows and columns\n",
    "    rows_count = df_check[name].count()\n",
    "    cols_count = len(df_check[name].columns)\n",
    "    if rows_count < 1:\n",
    "        raise ValueError(f'Data quality check failed! <{name}> contains 0 rows.')\n",
    "    if cols_count < 2:\n",
    "        raise ValueError(f'Data quality check failed! <{name}> contains 0 columns.')\n",
    "    print(f'Data Quality Checks passed successfully. <{name}> contains <{rows_count}> rows and <{cols_count}> columns.\\n')\n",
    "    found_tables.append(name)\n",
    "\n",
    "# sorting both the lists \n",
    "star_schema_tables.sort() \n",
    "found_tables.sort() \n",
    "# check part of star schema \n",
    "if star_schema_tables != found_tables:\n",
    "    raise ValueError('Tables not identical to star schema.')\n",
    "print ('All tables from star schema exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|weekday|count(1)|\n",
      "+-------+--------+\n",
      "|      1|   66873|\n",
      "|      6|   74947|\n",
      "|      3|   85483|\n",
      "|      5|  108968|\n",
      "|      4|  121404|\n",
      "|      2|   69165|\n",
      "|      0|   66645|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create tables\n",
    "immigrations_table.createOrReplaceTempView(\"immigrations_table\")\n",
    "date_table.createOrReplaceTempView(\"date_table\")\n",
    "country_temperatures_table.createOrReplaceTempView(\"country_temperatures_table\")\n",
    "states_table.createOrReplaceTempView(\"states_table\")\n",
    "cities_table.createOrReplaceTempView(\"cities_table\")\n",
    "us_state_temperatures_table.createOrReplaceTempView(\"us_state_temperatures_table\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT weekday, count(*) \n",
    "FROM immigrations_table i\n",
    "JOIN date_table d\n",
    "ON i.arrdate_ts = d.date\n",
    "JOIN cities_table c\n",
    "ON i.i94port = c.city_id\n",
    "WHERE c.state_code = 'FL'\n",
    "GROUP BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "*Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fact Table: Immigrations\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| ARRDATE | Arrival Date in the USA |\n",
    "| DEPDATE | Departure Date from the USA |\n",
    "| I94PORT | Port of arrival |\n",
    "| I94MODE | Mode of transportation |\n",
    "| AIRLINE | Airline used to arrive in U.S. |\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. |\n",
    "| I94YR | 4 digit year |\n",
    "| I94MON | Numeric month |\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|\n",
    "| I94VISA | Visa codes collapsed into three categories|\n",
    "| I94BIR | Age of Respondent in Years |\n",
    "| I94CIT | country of birth of the immigrant |\n",
    "| I94RES | resident country of the immigrant |\n",
    "| I94ADDR | State Code of arrival |\n",
    "| GENDER | Non-immigrant sex |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dim Table: States\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| state_code | US state code |\n",
    "| state | US state |\n",
    "| male_population | Number of male population |\n",
    "| female_population | Number of female population |\n",
    "| population | Number of total population |\n",
    "| number_of_veterans | Number of veterans in the city |\n",
    "| foreign-born | Number of citizens that were not born in the city |\n",
    "| american_indian_and_alaska_native | Number of american indian and alaska native people |\n",
    "| asian | Number of asian people |\n",
    "| black_or_african_american | Number of black or african american people |\n",
    "| hispanic_or_latino | Number of black or hispanic or latino people |\n",
    "| white | Number of white people |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dim Table: Dates\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| date | Date as YYYY-MM-DD |\n",
    "| day | Day |\n",
    "| month | Month |\n",
    "| year | Year |\n",
    "| week | Number of week of year |\n",
    "| weekday | Number of day of week |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dim Table: Cities\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| city_id | 3 letter code for the city |\n",
    "| city | City name |\n",
    "| state_code | State code |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dim Table: Country_temperatures\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| country | Country name |\n",
    "| year | Year |\n",
    "| month | Month |\n",
    "| AverageTemperature | Average Temperature of country for a specific month |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dim Table: US_state_temperatures\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| state | State name |\n",
    "| year | Year |\n",
    "| month | Month |\n",
    "| AverageTemperature | Average Temperature of US State for a specific month |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "**Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "For this project I used standardized tools for data processing and services from AWS.\n",
    "\n",
    "In particular it concerns:\n",
    "* Apache Spark: A unified analytics engine for large-scale data processing which works well with big data sources. It is easy to use because there are tools to use it directly in your Jupyter Notebook, i.e. by using the Data Frame API PySpark. You can use it for SQL-queries, streaming and complex analytics.\n",
    "\n",
    "* S3: With S3, AWS offers us an inexpensive, easy-to-use storage solution that is characterized by scalability, high availability, security and performance.\n",
    "\n",
    "* EMR: It is a cloud-based platform for large data volumes. EMR is characterized by the fact that large data volumes can be processed quickly and cost-effectively with Spark. Should performance requirements increase, the service can be easily extended.\n",
    "\n",
    "**Propose how often the data should be updated and why.**\n",
    "From the immigration data you can see that we will receive a new file every month. Therefore, it will make sense to start the data pipeline once a month.\n",
    "\n",
    "**Write a description of how you would approach the problem differently under the following scenarios**\n",
    " * The data was increased by 100x.\n",
    "\n",
    "AWS's flexible services make it easy to increase storage capacity on the S3 bucket and improve performance with additional processing nodes in EMR. \n",
    "\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "If the update requirements change, it would be advisable to use orchestration services such as Apache Airflow. DAGs can be executed regularly and thus keep the database up-to-date.\n",
    "\n",
    " * The database needed to be accessed by 100+ people.\n",
    "\n",
    "As described in the first point, the services used by AWS offer the simple possibility of scalability. Additional nodes for processing requests can be added quickly and easily to accommodate a larger user base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
